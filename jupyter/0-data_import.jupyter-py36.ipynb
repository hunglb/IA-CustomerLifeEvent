{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Import from a Remote Db2 Warehouse on Cloud Instance\n",
    "\n",
    "## Overview \n",
    "\n",
    "This notebook provides an example which connects to a Db2 Warehouse service running on IBM Cloud.  It includes the steps required to set up the data connection within IBM Cloud Private for Data to the service.  In this example, the data is stored in three different tables within Db2 Warehouse representing different business unit geographies which control the data.  The primary steps in the flow include:\n",
    "\n",
    "- Read the data from the 3 tables in the cloud database.\n",
    "- Normalize the data columns so that all 3 geographies have a common format.\n",
    "- Merge the three tables into a single data set.\n",
    "- Output the resulting data set to a CSV file for downstream processing.\n",
    "\n",
    "## Create a Db2 Warehouse on Cloud Instance and Set Up Connections\n",
    "\n",
    "A tutorial for creating a Db2 Warehouse instance on IBM Cloud is available and should be followed prior to running this notebook.  It includes steps to configure connections from this IBM Cloud Private for Data environment to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Data Sets\n",
    "\n",
    "After the database has been configured and connections to it established, datasets can be added to the project.  First, from the main page of this project, select __\"Data sets\"__ and then __\"Add Data Set\"__.  \n",
    "\n",
    "---\n",
    "<img src=\"../misc/images/add_data_set.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "On the next menu, select __\"Remote Data Set\"__, __\"Browse\"__, and then your database name.  The three tables in the example database (EVENTS_EUR, EVENTS_USE, EVENTS_USW) will each need to be selected and assigned remote data set names of EUR, USE, and USW respectively.  These names will be used in the notebook's python code to read in the cooresponding table data.\n",
    "\n",
    "<img src=\"../misc/images/remote_data_set.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of the data sets have been added to your project, you will see them listed as part of the _\"Data Sets\"_ assets as shown below.  You can now run the rest of this notebook to read the data from the database tables, join it into a single table, and write it out to a CSV file for processing in the remaining notebooks in this industry accelerator.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"../misc/images/final_data_sets.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Materials, provided under license. <br>\n",
    "Licensed Materials - Property of IBM. <br>\n",
    "Â© Copyright IBM Corp. 2019. All Rights Reserved. <br>\n",
    "US Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.<br>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Remote Database and Read the Data\n",
    "\n",
    "The following three cells connect the the DB2 Warehouse instance and pull the data from the three tables.  The code shown here is complete and will pull in the datasets configured as described above.  It was originally generated automatically for this notebook by following the __\"Find Data\"__ process as shown in the following diagram:\n",
    "\n",
    "\n",
    "---\n",
    "<img src=\"../misc/images/insert_dataframe.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CUSTOMER_ID USW_EVENT_DATE USW_EVENT_TYPE_ID\n",
      "0         1842     2017-01-03     INT_LOGIN_WEB\n",
      "1         1527     2017-01-03     INT_LOGIN_WEB\n",
      "2         1068     2017-01-03     INT_LOGIN_WEB\n",
      "3         1215     2017-01-03     INT_LOGIN_WEB\n",
      "4         1980     2017-01-03     INT_LOGIN_WEB\n"
     ]
    }
   ],
   "source": [
    "import dsx_core_utils, requests, jaydebeapi, os, io, sys\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "df1 = None\n",
    "dataSet = dsx_core_utils.get_remote_data_set_info('USW')\n",
    "dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource'])\n",
    "if (sys.version_info >= (3, 0)):\n",
    "  conn = jaydebeapi.connect(dataSource['driver_class'], dataSource['URL'], [dataSource['user'], dataSource['password']])\n",
    "else:\n",
    "  conn = jaydebeapi.connect(dataSource['driver_class'], [dataSource['URL'], dataSource['user'], dataSource['password']])\n",
    "query = 'select * from \"' + (dataSet['schema'] + '\".\"' if (len(dataSet['schema'].strip()) != 0) else '') +  dataSet['table'] + '\"'\n",
    "\n",
    "if (dataSet['query']):\n",
    "    query = dataSet['query']\n",
    "df1 = pd.read_sql(query, con=conn, parse_dates=['USW_EVENT_DATE'])\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CUSTOMER_ID USE_EVENT_DATE     USE_EVENT_TYPE_ID\n",
      "0         1900     2017-04-30  XFER_FUNDS_OUT_LARGE\n",
      "1         1900     2017-04-30            XCT_EQ_BUY\n",
      "2         1564     2017-04-30           XCT_EQ_SELL\n",
      "3         1900     2017-04-30           XCT_EQ_SELL\n",
      "4         1948     2017-04-30           XCT_EQ_SELL\n"
     ]
    }
   ],
   "source": [
    "import dsx_core_utils, requests, jaydebeapi, os, io, sys\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "df2 = None\n",
    "dataSet = dsx_core_utils.get_remote_data_set_info('USE')\n",
    "dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource'])\n",
    "if (sys.version_info >= (3, 0)):\n",
    "  conn = jaydebeapi.connect(dataSource['driver_class'], dataSource['URL'], [dataSource['user'], dataSource['password']])\n",
    "else:\n",
    "  conn = jaydebeapi.connect(dataSource['driver_class'], [dataSource['URL'], dataSource['user'], dataSource['password']])\n",
    "query = 'select * from \"' + (dataSet['schema'] + '\".\"' if (len(dataSet['schema'].strip()) != 0) else '') +  dataSet['table'] + '\"'\n",
    "\n",
    "if (dataSet['query']):\n",
    "    query = dataSet['query']\n",
    "df2 = pd.read_sql(query, con=conn, parse_dates=['USE_EVENT_DATE'])\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CUSTOMER_ID EUR_EVENT_DATE EUR_EVENT_TYPE_ID\n",
      "0         1109     2017-01-03     INT_LOGIN_WEB\n",
      "1         1166     2017-01-03     INT_LOGIN_WEB\n",
      "2         1007     2017-01-03     INT_LOGIN_WEB\n",
      "3         1679     2017-01-03     INT_LOGIN_WEB\n",
      "4         1439     2017-01-03     INT_LOGIN_WEB\n"
     ]
    }
   ],
   "source": [
    "import dsx_core_utils, requests, jaydebeapi, os, io, sys\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "df3 = None\n",
    "dataSet = dsx_core_utils.get_remote_data_set_info('EUR')\n",
    "dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource'])\n",
    "if (sys.version_info >= (3, 0)):\n",
    "  conn = jaydebeapi.connect(dataSource['driver_class'], dataSource['URL'], [dataSource['user'], dataSource['password']])\n",
    "else:\n",
    "  conn = jaydebeapi.connect(dataSource['driver_class'], [dataSource['URL'], dataSource['user'], dataSource['password']])\n",
    "query = 'select * from \"' + (dataSet['schema'] + '\".\"' if (len(dataSet['schema'].strip()) != 0) else '') +  dataSet['table'] + '\"'\n",
    "\n",
    "if (dataSet['query']):\n",
    "    query = dataSet['query']\n",
    "df3 = pd.read_sql(query, con=conn, parse_dates=['EUR_EVENT_DATE'])\n",
    "print(df3.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Tables\n",
    "\n",
    "This code sequence renames the columns of each of the tables to a uniform schema and then concatenates them into a single dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Data set:\n",
      "   CUSTOMER_ID EVENT_DATE  EVENT_TYPE_ID\n",
      "0         1842 2017-01-03  INT_LOGIN_WEB\n",
      "1         1527 2017-01-03  INT_LOGIN_WEB\n",
      "2         1068 2017-01-03  INT_LOGIN_WEB\n",
      "3         1215 2017-01-03  INT_LOGIN_WEB\n",
      "4         1980 2017-01-03  INT_LOGIN_WEB\n"
     ]
    }
   ],
   "source": [
    "df1.rename({'USW_EVENT_DATE': 'EVENT_DATE', 'USW_EVENT_TYPE_ID': 'EVENT_TYPE_ID'}, axis='columns', inplace=True)\n",
    "df2.rename({'USE_EVENT_DATE': 'EVENT_DATE', 'USE_EVENT_TYPE_ID': 'EVENT_TYPE_ID'}, axis='columns', inplace=True)\n",
    "df3.rename({'EUR_EVENT_DATE': 'EVENT_DATE', 'EUR_EVENT_TYPE_ID': 'EVENT_TYPE_ID'}, axis='columns', inplace=True)\n",
    "frames = [df1, df2, df3]\n",
    "df_raw = pd.concat(frames)\n",
    "print(\"Merged Data set:\")\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the Final Dataset\n",
    "\n",
    "Finally we output the dataset into a CSV file which is stored locally for processing in the subsequent steps of this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.to_csv('../datasets/events_combined.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
